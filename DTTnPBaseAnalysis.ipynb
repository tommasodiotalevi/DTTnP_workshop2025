{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT Tag-and-Probe (TnP) analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook features a simplified version of the Tag-and-Probe analysis of the CMS Drift Tubes (DT) for detector performance studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "distributed = False ## Default True - Enable DASK parallelisation\n",
    "MT = True ## Multi-threading - Disabled by default if distributed is True\n",
    "\n",
    "if distributed:\n",
    "    sched_port = 21901 ## Change this from the DASK cluster information panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Dask cluster configuration\n",
    "\n",
    "The following part configures the Dask cluster, that we launched previously in the JupyterLab interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if distributed:  \n",
    "    from dask.distributed import Client\n",
    "    \n",
    "    client = Client(\"localhost:\" + str(sched_port))\n",
    "\n",
    "    client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.restart() #Execute this only to restart the workers (to relaunch the notebook, for example#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the CMS X509 proxy (created using VOMS) in the Dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    from distributed.diagnostics.plugin import UploadFile\n",
    "    \n",
    "    client.register_plugin(UploadFile(\"./proxy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the proxy environmental variables in the Dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_proxy(dask_worker):\n",
    "    import os\n",
    "    import shutil\n",
    "    working_dir = dask_worker.local_directory\n",
    "    proxy_name = 'proxy'\n",
    "    os.environ['X509_USER_PROXY'] = working_dir + '/' + proxy_name\n",
    "    os.environ['X509_CERT_DIR']=\"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "    return os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    client.run(set_proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules and local X509 proxy configuration\n",
    "\n",
    "Import necessary libraries, setup the plotting tools, and configure the local X509 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going Multi-threaded\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import os\n",
    "import json\n",
    "\n",
    "from python import TnPTools\n",
    "import configparser\n",
    "\n",
    "#Enable multi-threading - Not possible if using RDF::Filter (nEvents != -1)\n",
    "if distributed != True and MT == True:\n",
    "    print(\"Going Multi-threaded\")\n",
    "    ROOT.ROOT.EnableImplicitMT()\n",
    "\n",
    "num_stations = 4\n",
    "num_wheels = 5\n",
    "num_sectors = 14\n",
    "\n",
    "# Loading PROXY locally\n",
    "os.environ['X509_USER_PROXY'] = \"proxy\"\n",
    "os.environ['X509_CERT_DIR'] = \"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift Tubes TnP analysis\n",
    "\n",
    "The following cells perform the DT Tag-and-Probe analysis ([CMS-DP-2023-049](https://cds.cern.ch/record/2868786?ln=en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the .INI file containing all the analysis variables initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./config/config_RDF.ini']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = './config/config_RDF.ini'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we open the JSON histogram settings file (which location is defined in the INI file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['Histogram']['bookFileName']) as f:\n",
    "    histo_settings = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every element in the inputFiles key in the config file, add the file to the `inputFiles` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFiles = []\n",
    "for i in config['Data']['inputFiles'].split(','):\n",
    "    if '.root' not in config['Data']['inputFiles']:\n",
    "        if config.has_option('Data','readRedirector'):\n",
    "            listdir = os.popen(\"xrdfs \" + config['Data']['readRedirector'] + \" ls \" + i).read()\n",
    "            inputFiles += sorted([\"root://{}/{}{}\".format(config['Data']['readRedirector'], i, f.split(\"/\")[-1]) for f in listdir.split(\"\\n\") if f.endswith('.root')])  \n",
    "        else:\n",
    "            inputFiles += sorted([os.path.join(i, f) for f in os.listdir(i) if f.endswith('.root')])\n",
    "    else:\n",
    "        if config.has_option('Data','readRedirector'):\n",
    "            if os.popen(\"xrdfs \" + config['Data']['readRedirector'] + \" ls \" + i).read() != \"\":\n",
    "                inputFiles.append(\"root://\" + config['Data']['readRedirector'] + \"/\" + i)\n",
    "        else:\n",
    "            if not os.path.exists(i):\n",
    "                print(\"\\033[91m File \" + i + \" not Found! \\033[0m\")\n",
    "                continue\n",
    "            inputFiles.append(i)\n",
    "tree_name = config['Data']['treeName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['root://eosuser.cern.ch//eos/user/t/todiotal/DTTnP_example/nano_mu_0.root']\n"
     ]
    }
   ],
   "source": [
    "print(inputFiles)  # Only one example Muon NanoAOD file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push and set auxiliary files\n",
    "\n",
    "The following cells upload the tools python module in the Dask workers (moving them in the worker's home folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./python/TnPTools.py'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['DataFrame']['TnPTools']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    client.register_plugin(UploadFile(config_file))\n",
    "    client.register_plugin(UploadFile(config['DataFrame']['TnPTools']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_auxiliary_files(dask_worker):\n",
    "    import os\n",
    "    import shutil\n",
    "    working_dir = dask_worker.local_directory\n",
    "    os.mkdir(working_dir + '/../../../config/')\n",
    "    os.mkdir(working_dir + '/../../../python/')\n",
    "    shutil.copyfile(working_dir + \"/\" + config_file.split(\"/\")[-1] , working_dir + '/../../../config/' + config_file.split(\"/\")[-1])\n",
    "    shutil.copyfile(working_dir + \"/\" + config['DataFrame']['TnPTools'].split(\"/\")[-1] , working_dir + '/../../../python/' + config['DataFrame']['TnPTools'].split(\"/\")[-1])\n",
    "    return os.listdir(\"/srv/config/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    client.run(set_auxiliary_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare custom functions in RDF\n",
    "\n",
    "Declare to the ROOT interpreter the header file which contains custom defined functions: this is done inside an inizialization function, which is necessary in the case of distributed execution (it is executed at the beginning of each task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./interface/df_tools.h\n"
     ]
    }
   ],
   "source": [
    "text_file = open(config['DataFrame']['customFunctions'], \"r\")\n",
    "data = text_file.read()\n",
    "print(config['DataFrame']['customFunctions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./interface/df_tools.h\n"
     ]
    }
   ],
   "source": [
    "text_file = open(config['DataFrame']['customFunctions'], \"r\")\n",
    "data = text_file.read()\n",
    "print(config['DataFrame']['customFunctions'])\n",
    "\n",
    "def my_initialization_function():\n",
    "    ROOT.gInterpreter.AddIncludePath(\"/usr/lib/boost_1_77_0\") #/opt/conda/include\n",
    "    ROOT.gInterpreter.Declare('const string configFile = \"{}\";'.format(config_file))\n",
    "    #ROOT.gInterpreter.Declare('{}'.format(data))\n",
    "    ROOT.gInterpreter.Declare(f\"\"\"\n",
    "        R__LOAD_LIBRARY(libPhysics);\n",
    "        {data}\n",
    "    \"\"\")\n",
    "\n",
    "if distributed:\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "else:\n",
    "    my_initialization_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame creation and execution on Dask workers\n",
    "\n",
    "After all the preliminary steps, we are now ready to **perform the analysis** using ROOT RDataFrame (in distributed mode).\\\n",
    "**The documentation for the usage of distributed RDF is available [here](https://root.cern/doc/v628/classROOT_1_1RDataFrame.html#distrdf).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set the granularity of the execution: how many partitions (i.e. tasks) we want our computation to run in. There is no fixed rule, this configuration depens on the analysis and the input file size.\n",
    "In our example, we set the `npartitions` to 3 times the number of Dask workers we created in our Cluster (such information can be retrieved using the `dask.distributed` Client API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    numWorkers= len(client.scheduler_info()['workers'])\n",
    "    npartitions = 3 * numWorkers\n",
    "    print(\"Number of workers is: {}\".format(numWorkers))\n",
    "    print(\"Number of total partitions is: {}\".format(npartitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the branches we are going to use as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputBranches = [\"run\", \"nmuon\", \"muon_pt\", \"muon_eta\", \"muon_phi\", \n",
    "                \"muon_charge\", \"muon_isRPC\", \"muon_isTracker\", \"muon_isTight\", \n",
    "                \"muon_trk_dz\", \"muon_pfIso04\", \"muon_trk_origAlgo\", \n",
    "                \"muon_trk_numberOfValidPixelHits\", \"muon_trk_numberOfValidTrackerLayers\",\n",
    "                \"muon_rpcMu_numberOfMatchedRPCLayers\", \"muon_trkMu_stationMask\", \n",
    "                \"muon_firesIsoTrig\", \"muon_firesTrig\", \"ndtSegment\", \n",
    "                \"muon_matches_station\", \"muon_matches_wheel\",\n",
    "                \"muon_matches_sector\", \"muon_matches_x\", \"muon_matches_y\", \"muon_matches_edgeX\",\n",
    "                \"muon_matches_edgeY\", \"muon_matches_begin\", \"muon_matches_end\", \"dtSegment_wheel\", \"dtSegment_sector\", \"dtSegment_station\",\n",
    "                \"dtSegment_seg4D_posLoc_x\", \"dtSegment_seg4D_posLoc_y\", \"dtSegment_seg2D_phi_t0\", \"dtSegment_seg4D_hasPhi\", \"dtSegment_seg4D_hasZed\",\n",
    "                \"dtSegment_seg2D_phi_nHits\", \"dtSegment_seg2D_z_nHits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dataframe (classic or distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TClass::Init>: no dictionary for class edm::Hash<1> is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ParameterSetBlob is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessHistory is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessConfiguration is available\n",
      "Warning in <TClass::Init>: no dictionary for class pair<edm::Hash<1>,edm::ParameterSetBlob> is available\n"
     ]
    }
   ],
   "source": [
    "if distributed:\n",
    "    df = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame(tree_name, inputFiles, npartitions=npartitions, daskclient=client) #len(inputFileName_list)\n",
    "else:\n",
    "    df = ROOT.RDataFrame(tree_name, inputFiles, inputBranches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the probe and pair variables (using the logic defined in the header file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.Define(\"probeIntVars\", \"setProbeVars<Int_t>(nmuon, muon_charge, muon_isTight, muon_isRPC, muon_firesTrig, muon_firesIsoTrig, \\\n",
    "               muon_isTracker, muon_trk_numberOfValidPixelHits, muon_trk_numberOfValidTrackerLayers, muon_rpcMu_numberOfMatchedRPCLayers, \\\n",
    "               muon_trk_origAlgo, muon_pfIso04, muon_pt, muon_eta, muon_phi, muon_trk_dz)\")\\\n",
    "       .Define(\"probeFloatVars\", \"setProbeVars<Float_t>(nmuon, muon_charge, muon_isTight, muon_isRPC, muon_firesTrig, muon_firesIsoTrig, \\\n",
    "               muon_isTracker, muon_trk_numberOfValidPixelHits, muon_trk_numberOfValidTrackerLayers, muon_rpcMu_numberOfMatchedRPCLayers, \\\n",
    "               muon_trk_origAlgo, muon_pfIso04, muon_pt, muon_eta, muon_phi, muon_trk_dz)\")\\\n",
    "       .Define(\"probeNPixelHits\", \"probeIntVars[0]\")\\\n",
    "       .Define(\"probeNTrkLayers\", \"probeIntVars[1]\")\\\n",
    "       .Define(\"probeNRPCLayers\", \"probeIntVars[2]\")\\\n",
    "       .Define(\"probeOrigAlgo\", \"probeIntVars[3]\")\\\n",
    "       .Define(\"pair_tagID\", \"probeIntVars[4]\")\\\n",
    "       .Define(\"pair_probeID\", \"probeIntVars[5]\")\\\n",
    "       .Define(\"probeReliso\", \"probeFloatVars[0]\")\\\n",
    "       .Define(\"pairMass\", \"probeFloatVars[1]\")\\\n",
    "       .Define(\"probePt\", \"probeFloatVars[2]\")\\\n",
    "       .Define(\"pairDr\", \"probeFloatVars[3]\")\\\n",
    "       .Define(\"pairDz\", \"probeFloatVars[4]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the segment efficiency variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.Define(\"segmentMapFloatEffVars\", \"segmentEfficiency<Float_t>(pair_probeID, muon_rpcMu_numberOfMatchedRPCLayers, run, muon_pt, muon_eta, muon_phi, muon_charge, muon_trkMu_stationMask, ndtSegment, muon_matches_station, muon_matches_wheel, muon_matches_sector, muon_matches_x, muon_matches_y, muon_matches_edgeX, muon_matches_edgeY, muon_matches_begin, muon_matches_end, dtSegment_wheel, dtSegment_sector, dtSegment_station, dtSegment_seg4D_posLoc_x, dtSegment_seg4D_posLoc_y, dtSegment_seg2D_phi_t0, dtSegment_seg4D_hasPhi, dtSegment_seg4D_hasZed, dtSegment_seg2D_phi_nHits, dtSegment_seg2D_z_nHits)\")\\\n",
    "       .Define(\"segmentMapIntEffVars\", \"segmentEfficiency<Int_t>(pair_probeID, muon_rpcMu_numberOfMatchedRPCLayers, run, muon_pt, muon_eta, muon_phi, muon_charge, muon_trkMu_stationMask, ndtSegment, muon_matches_station, muon_matches_wheel, muon_matches_sector, muon_matches_x, muon_matches_y, muon_matches_edgeX, muon_matches_edgeY, muon_matches_begin, muon_matches_end, dtSegment_wheel, dtSegment_sector, dtSegment_station, dtSegment_seg4D_posLoc_x, dtSegment_seg4D_posLoc_y, dtSegment_seg2D_phi_t0, dtSegment_seg4D_hasPhi, dtSegment_seg4D_hasZed, dtSegment_seg2D_phi_nHits, dtSegment_seg2D_z_nHits)\")\\\n",
    "       .Define(\"probeMuonEta\", \"segmentMapFloatEffVars[\\\"probeMuonEta\\\"]\")\\\n",
    "       .Define(\"nMatchInOtherCh\", \"segmentMapIntEffVars[\\\"nMatchInOtherCh\\\"]\")\\\n",
    "       .Define(\"effSecVsWhsecBinOnepassd\", \"segmentMapFloatEffVars[\\\"effSecVsWhAllOnesecpassd\\\"]\")\\\n",
    "       .Define(\"effSecVsWhsecBinOnetotal\", \"segmentMapFloatEffVars[\\\"effSecVsWhAllOnesectotal\\\"]\")\\\n",
    "       .Define(\"effSecVsWhwhBinOnepassd\", \"segmentMapFloatEffVars[\\\"effSecVsWhAllOnewhpassd\\\"]\")\\\n",
    "       .Define(\"effSecVsWhwhBinOnetotal\", \"segmentMapFloatEffVars[\\\"effSecVsWhAllOnewhtotal\\\"]\")\\\n",
    "       .Define(\"effSecVsWhsecBinTwopassd\", \"segmentMapFloatEffVars[\\\"effSecVsWhAllTwosecpassd\\\"]\")\\\n",
    "       .Define(\"effSecVsWhsecBinTwototal\", \"segmentMapFloatEffVars[\\\"effSecVsWhAllTwosectotal\\\"]\")\\\n",
    "       .Define(\"effSecVsWhwhBinTwopassd\", \"segmentMapFloatEffVars[\\\"effSecVsWhAllTwowhpassd\\\"]\")\\\n",
    "       .Define(\"effSecVsWhwhBinTwototal\", \"segmentMapFloatEffVars[\\\"effSecVsWhAllTwowhtotal\\\"]\")\n",
    "\n",
    "for i in range(num_stations):\n",
    "    df = df.Define(\"effAccVsEtaMB{}passd\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccVsEtaMB{}passd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccVsEtaMB{}total\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccVsEtaMB{}total\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccPhiVsEtaPlusMB{}etapassd\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccPhiVsEtaPlusMB{}etapassd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccPhiVsEtaPlusMB{}etatotal\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccPhiVsEtaPlusMB{}etatotal\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccPhiVsEtaPlusMB{}phipassd\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccPhiVsEtaPlusMB{}phipassd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccPhiVsEtaPlusMB{}phitotal\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccPhiVsEtaPlusMB{}phitotal\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccPhiVsEtaMinusMB{}etapassd\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccPhiVsEtaMinusMB{}etapassd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccPhiVsEtaMinusMB{}etatotal\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccPhiVsEtaMinusMB{}etatotal\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccPhiVsEtaMinusMB{}phipassd\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccPhiVsEtaMinusMB{}phipassd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccPhiVsEtaMinusMB{}phitotal\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccPhiVsEtaMinusMB{}phitotal\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccVsPhiPlusMB{}passd\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccVsPhiPlusMB{}passd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccVsPhiPlusMB{}total\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccVsPhiPlusMB{}total\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccVsPhiMinusMB{}passd\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccVsPhiMinusMB{}passd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccVsPhiMinusMB{}total\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccVsPhiMinusMB{}total\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccVsPtMB{}passd\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccVsPtMB{}passd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effAccVsPtMB{}total\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effAccVsPtMB{}total\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effVsPtMB{}passd\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effVsPtMB{}passd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effVsPtMB{}total\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effVsPtMB{}total\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effSecVsWhMB{}secmupassd\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effSecVsWhMB{}secmupassd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effSecVsWhMB{}secmutotal\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effSecVsWhMB{}secmutotal\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effSecVsWhMB{}whmupassd\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effSecVsWhMB{}whmupassd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effSecVsWhMB{}whmutotal\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"effSecVsWhMB{}whmutotal\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effVsNHitsPhiMB{}passd\".format(str(i+1)), \"segmentMapIntEffVars[\\\"effVsNHitsPhiMB{}passd\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"effVsNHitsPhiMB{}total\".format(str(i+1)), \"segmentMapIntEffVars[\\\"effVsNHitsPhiMB{}total\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"probePtMB{}\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"probePtMB{}\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"probeEtaMB{}\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"probeEtaMB{}\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"probePhiMB{}\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"probePhiMB{}\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"probePt_hasPhiMB{}\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"probePt_hasPhiMB{}\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"probeDrVsPtMB{}\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"probeDrVsPtMB{}\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"probeDxVsPtMB{}\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"probeDxVsPtMB{}\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"probeDyVsPtMB{}\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"probeDyVsPtMB{}\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"t0MB{}secBin\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"t0MB{}secBin\\\"]\".format(str(i+1)))\\\n",
    "       .Define(\"t0MB{}t0\".format(str(i+1)), \"segmentMapFloatEffVars[\\\"t0MB{}t0\\\"]\".format(str(i+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Book the histograms based on the informations provided in the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booked histo: probeNPixelHits\n",
      "Booked histo: probeNTrkLayers\n",
      "Booked histo: probeNRPCLayers\n",
      "Booked histo: probeOrigAlgo\n",
      "Booked histo: probeReliso\n",
      "Booked histo: pairMass\n",
      "Booked histo: pairDz\n",
      "Booked histo: effAccVsEtaMB1\n",
      "Booked histo: probePtMB1\n",
      "Booked histo: probeEtaMB1\n",
      "Booked histo: probePhiMB1\n",
      "Booked histo: effAccVsPhiPlusMB1\n",
      "Booked histo: effAccVsPhiMinusMB1\n",
      "Booked histo: effAccVsPtMB1\n",
      "Booked histo: effVsPtMB1\n",
      "Booked histo: effVsNHitsPhiMB1\n",
      "Booked histo: effAccVsEtaMB2\n",
      "Booked histo: probePtMB2\n",
      "Booked histo: probeEtaMB2\n",
      "Booked histo: probePhiMB2\n",
      "Booked histo: effAccVsPhiPlusMB2\n",
      "Booked histo: effAccVsPhiMinusMB2\n",
      "Booked histo: effAccVsPtMB2\n",
      "Booked histo: effVsPtMB2\n",
      "Booked histo: effVsNHitsPhiMB2\n",
      "Booked histo: effAccVsEtaMB3\n",
      "Booked histo: probePtMB3\n",
      "Booked histo: probeEtaMB3\n",
      "Booked histo: probePhiMB3\n",
      "Booked histo: effAccVsPhiPlusMB3\n",
      "Booked histo: effAccVsPhiMinusMB3\n",
      "Booked histo: effAccVsPtMB3\n",
      "Booked histo: effVsPtMB3\n",
      "Booked histo: effVsNHitsPhiMB3\n",
      "Booked histo: effAccVsEtaMB4\n",
      "Booked histo: probePtMB4\n",
      "Booked histo: probeEtaMB4\n",
      "Booked histo: probePhiMB4\n",
      "Booked histo: effAccVsPhiPlusMB4\n",
      "Booked histo: effAccVsPhiMinusMB4\n",
      "Booked histo: effAccVsPtMB4\n",
      "Booked histo: effVsPtMB4\n",
      "Booked histo: effVsNHitsPhiMB4\n",
      "Booked histo: probePtVsPairDr\n",
      "Booked histo: nOtherMatchedChVsEta\n",
      "Booked histo: effSecVsWhAllOne\n",
      "Booked histo: effSecVsWhAllTwo\n",
      "Booked histo: effAccPhiVsEtaPlusMB1\n",
      "Booked histo: effAccPhiVsEtaMinusMB1\n",
      "Booked histo: probeDrVsPtMB1\n",
      "Booked histo: probeDxVsPtMB1\n",
      "Booked histo: probeDyVsPtMB1\n",
      "Booked histo: effSecVsWhMB1\n",
      "Booked histo: t0MB1\n",
      "Booked histo: effAccPhiVsEtaPlusMB2\n",
      "Booked histo: effAccPhiVsEtaMinusMB2\n",
      "Booked histo: probeDrVsPtMB2\n",
      "Booked histo: probeDxVsPtMB2\n",
      "Booked histo: probeDyVsPtMB2\n",
      "Booked histo: effSecVsWhMB2\n",
      "Booked histo: t0MB2\n",
      "Booked histo: effAccPhiVsEtaPlusMB3\n",
      "Booked histo: effAccPhiVsEtaMinusMB3\n",
      "Booked histo: probeDrVsPtMB3\n",
      "Booked histo: probeDxVsPtMB3\n",
      "Booked histo: probeDyVsPtMB3\n",
      "Booked histo: effSecVsWhMB3\n",
      "Booked histo: t0MB3\n",
      "Booked histo: effAccPhiVsEtaPlusMB4\n",
      "Booked histo: effAccPhiVsEtaMinusMB4\n",
      "Booked histo: probeDrVsPtMB4\n",
      "Booked histo: probeDxVsPtMB4\n",
      "Booked histo: probeDyVsPtMB4\n",
      "Booked histo: effSecVsWhMB4\n",
      "Booked histo: t0MB4\n",
      "Booked histo: t0ProfileMB1\n",
      "Booked histo: t0ProfileMB2\n",
      "Booked histo: t0ProfileMB3\n",
      "Booked histo: t0ProfileMB4\n"
     ]
    }
   ],
   "source": [
    "booked_1Dhistos = {\n",
    "    'probeNPixelHits': 'probeNPixelHits',\n",
    "    'probeNTrkLayers': 'probeNTrkLayers',\n",
    "    'probeNRPCLayers': 'probeNRPCLayers',\n",
    "    'probeOrigAlgo': 'probeOrigAlgo',\n",
    "    'probeReliso': 'probeReliso',\n",
    "    'pairMass': 'pairMass',\n",
    "    'pairDz': 'pairDz'}\n",
    "\n",
    "for i in range(num_stations):\n",
    "    booked_1Dhistos[\"effAccVsEtaMB{}\".format(str(i+1))] = \"effAccVsEtaMB{}\".format(str(i+1))\n",
    "    booked_1Dhistos[\"probePtMB{}\".format(str(i+1))] = \"probePtMB{}\".format(str(i+1))\n",
    "    booked_1Dhistos[\"probeEtaMB{}\".format(str(i+1))] = \"probeEtaMB{}\".format(str(i+1))\n",
    "    booked_1Dhistos[\"probePhiMB{}\".format(str(i+1))] = \"probePhiMB{}\".format(str(i+1))\n",
    "    booked_1Dhistos[\"effAccVsPhiPlusMB{}\".format(str(i+1))] = \"effAccVsPhiPlusMB{}\".format(str(i+1))\n",
    "    booked_1Dhistos[\"effAccVsPhiMinusMB{}\".format(str(i+1))] = \"effAccVsPhiMinusMB{}\".format(str(i+1))\n",
    "    booked_1Dhistos[\"effAccVsPtMB{}\".format(str(i+1))] = \"effAccVsPtMB{}\".format(str(i+1))\n",
    "    booked_1Dhistos[\"effVsPtMB{}\".format(str(i+1))] = \"effVsPtMB{}\".format(str(i+1))\n",
    "    booked_1Dhistos[\"effVsNHitsPhiMB{}\".format(str(i+1))] = \"effVsNHitsPhiMB{}\".format(str(i+1))\n",
    "\n",
    "booked_2Dhistos = {\n",
    "    'probePtVsPairDr': ['probePt', 'pairDr'],\n",
    "    'nOtherMatchedChVsEta': ['probeMuonEta', 'nMatchInOtherCh'],\n",
    "    'effSecVsWhAllOne': ['effSecVsWhsecBinOne', 'effSecVsWhwhBinOne'],\n",
    "    'effSecVsWhAllTwo': ['effSecVsWhwhBinTwo', 'effSecVsWhsecBinTwo']\n",
    "}\n",
    "\n",
    "for i in range(num_stations):\n",
    "    booked_2Dhistos[\"effAccPhiVsEtaPlusMB{}\".format(str(i+1))] = [\"effAccPhiVsEtaPlusMB{}phi\".format(str(i+1)), \"effAccPhiVsEtaPlusMB{}eta\".format(str(i+1))]\n",
    "    booked_2Dhistos[\"effAccPhiVsEtaMinusMB{}\".format(str(i+1))] = [\"effAccPhiVsEtaMinusMB{}phi\".format(str(i+1)), \"effAccPhiVsEtaMinusMB{}eta\".format(str(i+1))]\n",
    "    booked_2Dhistos[\"probeDrVsPtMB{}\".format(str(i+1))] = [\"probePt_hasPhiMB{}\".format(str(i+1)), \"probeDrVsPtMB{}\".format(str(i+1))]\n",
    "    booked_2Dhistos[\"probeDxVsPtMB{}\".format(str(i+1))] = [\"probePt_hasPhiMB{}\".format(str(i+1)), \"probeDxVsPtMB{}\".format(str(i+1))]\n",
    "    booked_2Dhistos[\"probeDyVsPtMB{}\".format(str(i+1))] = [\"probePt_hasPhiMB{}\".format(str(i+1)), \"probeDyVsPtMB{}\".format(str(i+1))]\n",
    "    booked_2Dhistos[\"effSecVsWhMB{}\".format(str(i+1))] = [\"effSecVsWhMB{}secmu\".format(str(i+1)), \"effSecVsWhMB{}whmu\".format(str(i+1))]\n",
    "    booked_2Dhistos[\"t0MB{}\".format(str(i+1))] = [\"t0MB{}secBin\".format(str(i+1)), \"t0MB{}t0\".format(str(i+1))]\n",
    "\n",
    "booked_profile = {}\n",
    "\n",
    "for i in range(num_stations):\n",
    "    booked_profile[\"t0ProfileMB{}\".format(str(i+1))] = [\"t0MB{}secBin\".format(str(i+1)), \"t0MB{}t0\".format(str(i+1))]\n",
    "\n",
    "histograms = {}\n",
    "\n",
    "TnPTools.book_1Dhistos(df, histograms, booked_1Dhistos, histo_settings)\n",
    "TnPTools.book_2Dhistos(df, histograms, booked_2Dhistos, histo_settings)\n",
    "TnPTools.book_profile(df, histograms, booked_profile, histo_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cells (in distributed mode) are *lazy*, meaning they don't trigger any computational graph. **The cell below**, on the other hand, **starts the execution**. You can see the resource usage in the `Workers` page of the Dask LabExtension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Histo: probeNPixelHits\n",
      "Created Histo: probeNTrkLayers\n",
      "Created Histo: probeNRPCLayers\n",
      "Created Histo: probeOrigAlgo\n",
      "Created Histo: probeReliso\n",
      "Created Histo: pairMass\n",
      "Created Histo: pairDz\n",
      "Created Histo: effAccVsEtaMB1\n",
      "Created Histo: probePtMB1\n",
      "Created Histo: probeEtaMB1\n",
      "Created Histo: probePhiMB1\n",
      "Created Histo: effAccVsPhiPlusMB1\n",
      "Created Histo: effAccVsPhiMinusMB1\n",
      "Created Histo: effAccVsPtMB1\n",
      "Created Histo: effVsPtMB1\n",
      "Created Histo: effVsNHitsPhiMB1\n",
      "Created Histo: effAccVsEtaMB2\n",
      "Created Histo: probePtMB2\n",
      "Created Histo: probeEtaMB2\n",
      "Created Histo: probePhiMB2\n",
      "Created Histo: effAccVsPhiPlusMB2\n",
      "Created Histo: effAccVsPhiMinusMB2\n",
      "Created Histo: effAccVsPtMB2\n",
      "Created Histo: effVsPtMB2\n",
      "Created Histo: effVsNHitsPhiMB2\n",
      "Created Histo: effAccVsEtaMB3\n",
      "Created Histo: probePtMB3\n",
      "Created Histo: probeEtaMB3\n",
      "Created Histo: probePhiMB3\n",
      "Created Histo: effAccVsPhiPlusMB3\n",
      "Created Histo: effAccVsPhiMinusMB3\n",
      "Created Histo: effAccVsPtMB3\n",
      "Created Histo: effVsPtMB3\n",
      "Created Histo: effVsNHitsPhiMB3\n",
      "Created Histo: effAccVsEtaMB4\n",
      "Created Histo: probePtMB4\n",
      "Created Histo: probeEtaMB4\n",
      "Created Histo: probePhiMB4\n",
      "Created Histo: effAccVsPhiPlusMB4\n",
      "Created Histo: effAccVsPhiMinusMB4\n",
      "Created Histo: effAccVsPtMB4\n",
      "Created Histo: effVsPtMB4\n",
      "Created Histo: effVsNHitsPhiMB4\n",
      "Created Histo: probePtVsPairDr\n",
      "Created Histo: nOtherMatchedChVsEta\n",
      "Created Histo: effSecVsWhAllOne\n",
      "Created Histo: effSecVsWhAllTwo\n",
      "Created Histo: effAccPhiVsEtaPlusMB1\n",
      "Created Histo: effAccPhiVsEtaMinusMB1\n",
      "Created Histo: probeDrVsPtMB1\n",
      "Created Histo: probeDxVsPtMB1\n",
      "Created Histo: probeDyVsPtMB1\n",
      "Created Histo: effSecVsWhMB1\n",
      "Created Histo: t0MB1\n",
      "Created Histo: effAccPhiVsEtaPlusMB2\n",
      "Created Histo: effAccPhiVsEtaMinusMB2\n",
      "Created Histo: probeDrVsPtMB2\n",
      "Created Histo: probeDxVsPtMB2\n",
      "Created Histo: probeDyVsPtMB2\n",
      "Created Histo: effSecVsWhMB2\n",
      "Created Histo: t0MB2\n",
      "Created Histo: effAccPhiVsEtaPlusMB3\n",
      "Created Histo: effAccPhiVsEtaMinusMB3\n",
      "Created Histo: probeDrVsPtMB3\n",
      "Created Histo: probeDxVsPtMB3\n",
      "Created Histo: probeDyVsPtMB3\n",
      "Created Histo: effSecVsWhMB3\n",
      "Created Histo: t0MB3\n",
      "Created Histo: effAccPhiVsEtaPlusMB4\n",
      "Created Histo: effAccPhiVsEtaMinusMB4\n",
      "Created Histo: probeDrVsPtMB4\n",
      "Created Histo: probeDxVsPtMB4\n",
      "Created Histo: probeDyVsPtMB4\n",
      "Created Histo: effSecVsWhMB4\n",
      "Created Histo: t0MB4\n",
      "Created Histo: t0ProfileMB1\n",
      "Created Histo: t0ProfileMB2\n",
      "Created Histo: t0ProfileMB3\n",
      "Created Histo: t0ProfileMB4\n",
      "CPU times: user 1min 4s, sys: 1.74 s, total: 1min 5s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TnPTools.write_histos(histograms, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a ROOT file containing several histograms and plots. This last cell only saves a few of them as image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info in <TCanvas::Print>: png file ./output/pairMass.png has been created\n",
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: canvas\n",
      "Info in <TCanvas::Print>: png file ./output/effVsPt.png has been created\n",
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: canvas\n",
      "Info in <TCanvas::Print>: png file ./output/effAccVsEta.png has been created\n"
     ]
    }
   ],
   "source": [
    "file = ROOT.TFile.Open(\"./output/results.root\")\n",
    "\n",
    "pairMass = file.Get(\"pairMass\")\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Canvas\", 800, 600)\n",
    "pairMass.Draw()\n",
    "canvas.SaveAs(\"./output/pairMass.png\")\n",
    "\n",
    "effVsPtMB1 = file.Get(\"effVsPtMB1\")\n",
    "effVsPtMB2 = file.Get(\"effVsPtMB2\")\n",
    "effVsPtMB3 = file.Get(\"effVsPtMB3\")\n",
    "effVsPtMB4 = file.Get(\"effVsPtMB4\")\n",
    "\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Canvas\", 800, 600)\n",
    "canvas.Divide(2, 2)  # 2x2 grid\n",
    "canvas.cd(1)\n",
    "effVsPtMB1.Draw()\n",
    "canvas.cd(2)\n",
    "effVsPtMB2.Draw()\n",
    "canvas.cd(3)\n",
    "effVsPtMB3.Draw()\n",
    "canvas.cd(4)\n",
    "effVsPtMB4.Draw()\n",
    "\n",
    "canvas.SaveAs(\"./output/effVsPt.png\")\n",
    "\n",
    "effAccVsEtaMB1 = file.Get(\"effAccVsEtaMB1\")\n",
    "effAccVsEtaMB2 = file.Get(\"effAccVsEtaMB2\")\n",
    "effAccVsEtaMB3 = file.Get(\"effAccVsEtaMB3\")\n",
    "effAccVsEtaMB4 = file.Get(\"effAccVsEtaMB4\")\n",
    "\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Canvas\", 800, 600)\n",
    "canvas.Divide(2, 2)  # 2x2 grid\n",
    "canvas.cd(1)\n",
    "effAccVsEtaMB1.Draw()\n",
    "canvas.cd(2)\n",
    "effAccVsEtaMB2.Draw()\n",
    "canvas.cd(3)\n",
    "effAccVsEtaMB3.Draw()\n",
    "canvas.cd(4)\n",
    "effAccVsEtaMB4.Draw()\n",
    "\n",
    "canvas.SaveAs(\"./output/effAccVsEta.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see the execution time, running locally (on the JLab resources)? Let's try now distributed (turning on the `distributed` flag at the beginning of the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional**: move the ROOT file just created to a remote site storage (in this case the Legnaro Tier-2 storage) via Davix/https:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"davix-put ./output/results.root \" + \"davs://\" + config['Data']['writeRedirector'] + \"/store/user/todiotal/results.root\" + \" -E $X509_USER_PROXY --capath $X509_CERT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity kernel - Base ROOT 6.32.02 + CMSJMECalculator 0.2.0 + Correctionlib 2.6.1",
   "language": "python",
   "name": "singularity-kernel-root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
